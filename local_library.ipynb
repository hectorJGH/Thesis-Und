{"cells":[{"cell_type":"markdown","metadata":{"id":"mBlHTwVHt1Ni"},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_NUgY2GZfJl"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import LinearSegmentedColormap\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import make_circles, make_moons, make_swiss_roll\n","from sklearn import preprocessing\n","from functools import reduce #To use multiple np.kron\n","\n","from scipy.optimize import curve_fit\n","\n","import math\n"]},{"cell_type":"markdown","metadata":{"id":"pMr3MGypKrXJ"},"source":["# Datasets"]},{"cell_type":"markdown","metadata":{"id":"QI_N24b-gw-C"},"source":["## Spiral function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzKKV_U7KqLz"},"outputs":[],"source":["\n","# Function to generate a spiral\n","def make_spiral(phase, slope, noise_std, num_points, turns, label, random_seed=0):\n","    theta = np.linspace(0, 2 * np.pi * turns, num_points)\n","    r = slope *(theta ) # Spiral grows in radius with angle\n","\n","    # Add random noise in theta\n","    np.random.seed(random_seed)\n","    theta += np.random.normal(0, noise_std, num_points)\n","\n","    x = r * np.cos(theta + phase)\n","    y = r * np.sin(theta + phase)\n","\n","    # Add Gaussian noise\n","    #x += np.random.normal(0, noise_std, num_points)\n","    #y += np.random.normal(0, noise_std, num_points)\n","\n","    labels = np.full(num_points, label)\n","    return x, y, labels\n","\n","\n","def generate_spirals(num_points, random_seed, noise = 0.5, plot = False, test_size=0.25):\n","    # Parameters\n","    turns = 1  # Number of turns in the spiral\n","    slope = 1\n","\n","    # Generate two spirals\n","    x1, y1, labels1 = make_spiral(0, slope, noise, num_points, turns, label=0, random_seed=random_seed)\n","    x2, y2, labels2 = make_spiral(np.pi, slope, noise, num_points, turns, label=1, random_seed=random_seed+1)\n","\n","    # Create a DataFrame\n","    df = pd.DataFrame({\n","        'x': np.concatenate([x1, x2]),\n","        'y': np.concatenate([y1, y2]),\n","        'label': np.concatenate([labels1, labels2])\n","    })\n","\n","    # Scale it to a range\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    df[['x','y']] =  min_max_scaler.fit_transform(df[['x','y']])\n","\n","    # Split the data into training and test sets\n","    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42*random_seed)\n","\n","    train_arr = np.array(train_df[['x','y']])\n","    train_lb = np.array(train_df['label'])\n","    test_arr = np.array(test_df[['x','y']])\n","    test_lb = np.array(test_df['label'])\n","\n","    if plot:\n","        # Plot the training and test data for visualization\n","        plt.scatter(train_df['x'], train_df['y'], c=train_df['label'], label='Train Data', alpha=0.6)\n","        plt.scatter(test_df['x'], test_df['y'], c=test_df['label'], marker='x', label='Test Data', alpha=0.6)\n","        plt.legend()\n","        plt.title('Training and Test Data for Two Spirals')\n","\n","    return train_arr, train_lb, test_arr, test_lb\n","\n","\n","#num_points = 200\n","#random_seed = 1\n","#train_arr, train_lb, test_arr, test_lb, df = generate_spirals(num_points,random_seed, plot= True)"]},{"cell_type":"markdown","metadata":{"id":"hc-rHPq7ND6H"},"source":["## Moons"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YRmUWtYHMSf_"},"outputs":[],"source":["def generate_moons(num_points, random_seed,  noise = 0.1, plot = False, test_size=0.25):\n","    # Generate datasets\n","    X_moons, y_moons = make_moons(n_samples=num_points, noise=noise, random_state=random_seed)\n","\n","    # Scale it to a range\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    X_moons = min_max_scaler.fit_transform(X_moons)\n","\n","\n","    # Split the data into training and test sets\n","    X_train_moons, X_test_moons, y_train_moons, y_test_moons = train_test_split(\n","        X_moons, y_moons, test_size=test_size, random_state=42*random_seed\n","    )\n","\n","    if plot:\n","        # Plot the training and test data for visualization\n","        plt.scatter(\n","            X_train_moons[:, 0],\n","            X_train_moons[:, 1],\n","            c=y_train_moons,\n","            label=\"Train Data\",\n","            alpha=0.6,\n","        )\n","        plt.scatter(\n","            X_test_moons[:, 0],\n","            X_test_moons[:, 1],\n","            c=y_test_moons,\n","            marker=\"x\",\n","            label=\"Test Data\",\n","            alpha=0.6,\n","        )\n","        plt.legend()\n","        plt.title(\"Training and Test Data for Moons Dataset\")\n","        plt.show()\n","    return X_train_moons, y_train_moons, X_test_moons, y_test_moons\n","\n","#num_points = 200\n","#random_seed = 1\n","#X_train_moons, y_train_moons, X_test_moons, y_test_moons = generate_moons(num_points,random_seed, plot= True)"]},{"cell_type":"markdown","source":["## Circles"],"metadata":{"id":"64ps4OOlGIgX"}},{"cell_type":"code","source":["def generate_circles(num_points, random_seed,  noise = 0.1, plot = False, test_size=0.25, factor=0.5):\n","    # Generate datasets\n","    X_circles, y_circles = make_circles(n_samples=num_points, noise=noise, random_state=random_seed, factor=factor)\n","\n","    # Scale it between 0 and 1\n","    min_max_scaler = preprocessing.MinMaxScaler()\n","    X_circles = min_max_scaler.fit_transform(X_circles)\n","\n","    # Split the data into training and test sets\n","    X_train_circles, X_test_circles, y_train_circles, y_test_circles = train_test_split(\n","        X_circles, y_circles, test_size=test_size, random_state=42*random_seed)\n","\n","    if plot:\n","        # Plot the training and test data for visualization\n","        plt.scatter(\n","            X_train_circles[:, 0],\n","            X_train_circles[:, 1],\n","            c=y_train_circles,\n","            label=\"Train Data\",\n","            alpha=0.6,\n","        )\n","        plt.scatter(\n","            X_test_circles[:, 0],\n","            X_test_circles[:, 1],\n","            c=y_test_circles,\n","            marker=\"x\",\n","            label=\"Test Data\",\n","            alpha=0.6,\n","        )\n","        plt.legend()\n","        plt.title(\"Training and Test Data for Circles Dataset\")\n","        plt.show()\n","    return X_train_circles, y_train_circles, X_test_circles, y_test_circles"],"metadata":{"id":"B1ZZMFPY_WDY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Diminish size"],"metadata":{"id":"kPpkyfHGnizR"}},{"cell_type":"code","source":["def diminish_size(xy_array, scale = 0.5):\n","    if xy_array.shape[0] <=1:\n","      print('No diminish')\n","      return\n","    if xy_array.shape[1] != 2:\n","      print('Wrong dimension')\n","      return\n","    max_x = np.max(xy_array[:,0])\n","    min_x = np.min(xy_array[:,0])\n","    max_y = np.max(xy_array[:,1])\n","    min_y = np.min(xy_array[:,1])\n","    # Rescaling\n","    xy_array[:,0] = (xy_array[:,0] - min_x)/(max_x - min_x) * scale\n","    xy_array[:,1] = (xy_array[:,1] - min_y)/(max_y - min_y) * scale\n","    # Recentering\n","    xy_array[:,0] = xy_array[:,0] + (1-scale)/2\n","    xy_array[:,1] = xy_array[:,1] + (1-scale)/2\n","    return xy_array"],"metadata":{"id":"DPjxrGmLnqoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_pZ7tEWt7om"},"source":["# Quantum Feature Map"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_pbabXOBRnxg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737214564155,"user_tz":300,"elapsed":51,"user":{"displayName":"Hector Julian Gutierrez Hoyos","userId":"09358593715852133437"}},"outputId":"746e2c4f-ab25-4339-90d7-5365f392d3cd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":6}],"source":["def softmax_gaussian(x,dim_qm,beta): # It assumes data is between 0 and 1\n","    alpha =  np.arange(dim_qm) / (dim_qm-1)\n","    P = np.array([np.sqrt(np.exp(-beta*np.square(alpha-x_i)) / np.sum(np.exp(-beta*np.square(alpha-x_i)))) for x_i in x])\n","    #dimshape = [P.shape[1]]*P.shape[0]\n","    #print(P.shape)\n","    ##reduce(np.kron, P).reshape(dimshape) # reshape Generates error with np.outer among others\n","    Psi = reduce(np.kron, P)\n","    Psi = Psi / np.linalg.norm(Psi)\n","    return Psi\n","\n","#softmax_gaussian(train_arr[0],100,5)\n","\n","#print(softmax_gaussian(np.array([0.1,0.2]),7,5).shape)\n","#print(softmax_gaussian(np.array([1,2]),7,5))\n","np.linalg.norm(softmax_gaussian(np.array([1,5.2]),7,5))\n","\n","#beta = 5\n","#dim_qm = 100\n","#x = np.arange(dim_qm)\n","#alpha =  np.arange(dim_qm) / (dim_qm-1)\n","#P = np.array([np.sqrt(np.exp(-beta*np.square(alpha-x_i))/sum(np.exp(-beta*np.square(alpha-x_i)))) for x_i in x])\n","#reduce(np.kron, P)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dni0pErFMcUx"},"outputs":[],"source":["#math.factorial(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKzwtDg51GQa"},"outputs":[],"source":["#np.arange(6).reshape(2,3).T"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LT7g8xB0LVq6"},"outputs":[],"source":["def squeezed(phi_input,trunc_n, r):\n","    phi = phi_input #*np.pi\n","    Psi = 1/np.sqrt(np.cosh(r)) * np.array([np.sqrt(float(math.factorial(2*n)))\n","      /(float(2**n) * float(math.factorial(n)))*np.exp(n*(phi)*1j) * np.power(np.tanh(r),n)*(-1)**n for n in range(trunc_n)])\n","    #print(Psi.shape)\n","    Psi = Psi.T # To make the tensorial product along the right dimension\n","    #dimshape = [Psi.shape[1]]*Psi.shape[0]\n","    tensorial_prod = reduce(np.kron, Psi)\n","    return tensorial_prod / np.linalg.norm(tensorial_prod) ##.reshape(dimshape) # reshape Generates error with np.outer among others\n","\n","#squeezed(1,np.array([0,np.pi,np.pi/3]),7)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRP-Mim-RiQ7"},"outputs":[],"source":["#print(np.random.rand(*np.array([[1,2],[3,4]]).shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfzLR4YGLJsL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737214564157,"user_tz":300,"elapsed":41,"user":{"displayName":"Hector Julian Gutierrez Hoyos","userId":"09358593715852133437"}},"outputId":"aee63c95-d9ca-4086-d25b-b8872526fa9e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef coherent(x, trunc_n, gamma):\\n    # Assuming data is between 0 and 1\\n    theta = x*np.pi\\n    #theta = np.random.rand(*x.shape)*np.pi #np.arcsin(x)\\n    alpha = x* np.exp(1j*theta)\\n    Psi = np.array([np.exp(-gamma * abs(alpha)**2 /2) * alpha**n * np.float_power(gamma,n/2)/\\n                    np.sqrt(float(math.factorial(n))) for n in range(trunc_n)])\\n    Psi = Psi.T # To make the tensorial product along the right dimension\\n    tensor_prod = reduce(np.kron, Psi)\\n    #dimshape = [Psi.shape[1]]*Psi.shape[0]\\n    return tensor_prod/np.linalg.norm(tensor_prod)\\n\\n\\n\\nstate = coherent(np.array([0,0.2]),2,5)\\nprint(state.shape)\\nprint(state)\\nprint(np.linalg.norm(state))'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["\"\"\"\n","def coherent(x, trunc_n, gamma):\n","    # Assuming data is between 0 and 1\n","    theta = x*np.pi\n","    #theta = np.random.rand(*x.shape)*np.pi #np.arcsin(x)\n","    alpha = x* np.exp(1j*theta)\n","    Psi = np.array([np.exp(-gamma * abs(alpha)**2 /2) * alpha**n * np.float_power(gamma,n/2)/\n","                    np.sqrt(float(math.factorial(n))) for n in range(trunc_n)])\n","    Psi = Psi.T # To make the tensorial product along the right dimension\n","    tensor_prod = reduce(np.kron, Psi)\n","    #dimshape = [Psi.shape[1]]*Psi.shape[0]\n","    return tensor_prod/np.linalg.norm(tensor_prod)\n","\n","\n","\n","state = coherent(np.array([0,0.2]),2,5)\n","print(state.shape)\n","print(state)\n","print(np.linalg.norm(state))\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzjdeU_wI22B"},"outputs":[],"source":["def coherent(x,dim,beta):\n","    ''' Represents each state with a coherent state\n","    '''\n","    alpha = lambda x: x * (np.cos(np.pi * x) + 1j * np.sin(np.pi * x))\n","    basis = np.arange(dim)\n","    P = np.array([np.power(alpha(x) * np.sqrt(beta),n)/\n","                  np.sqrt(float(math.factorial(n))) for n in basis]).T\n","    Psi = reduce(np.kron, P)\n","    Psi = Psi / np.linalg.norm(Psi)\n","    return Psi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiZjxpBsgwkR"},"outputs":[],"source":["def onehot(y,y_set = [0,1]):\n","    '''Quantum Feature Map for discrete variables, i.e. labels\n","    y: label\n","    y_set: set of labels\n","    Each variable in y_set is an element of the basis'''\n","\n","    return np.array([int(i == y) for i in y_set])\n","\n","#onehot(0)"]},{"cell_type":"markdown","metadata":{"id":"T6h_tdkj8AXL"},"source":["# States and Matrices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sdb5WF-ovbjd"},"outputs":[],"source":["def pure_density(train_QFM):\n","    state = np.sum(train_QFM,axis=0)\n","    norm_state = state/np.linalg.norm(state)\n","    density = np.outer(norm_state,np.conjugate(norm_state))\n","    return density\n","\n","def pure_state(train_QFM):\n","    state = np.sum(train_QFM,axis=0)\n","    norm_state = state/np.linalg.norm(state)\n","    return norm_state\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"banV6gZN5Kz3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737214564991,"user_tz":300,"elapsed":868,"user":{"displayName":"Hector Julian Gutierrez Hoyos","userId":"09358593715852133437"}},"outputId":"29a3c4cc-4f85-4bfc-a9ac-6c88075394e5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'def mixed_density(train_QFM):\\n    Psi_sum=np.zeros((np.shape(train_QFM)[1], np.shape(train_QFM)[1]), dtype=np.complex128)\\n    for i in range(len(train_QFM)):\\n        norm_state = train_QFM[i]/np.linalg.norm(train_QFM[i])\\n        Psi_sum += np.outer(norm_state,np.conj(norm_state))\\n    #state = np.sum(train_QFM,axis=0)\\n    #norm_state = state/np.sum(state)\\n    density = Psi_sum/len(train_QFM)\\n    return density\\n\\nprint(np.shape(np.outer(np.zeros(3),np.zeros(3))))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["\"\"\"def mixed_density(train_QFM):\n","    Psi_sum=np.zeros((np.shape(train_QFM)[1], np.shape(train_QFM)[1]), dtype=np.complex128)\n","    for i in range(len(train_QFM)):\n","        norm_state = train_QFM[i]/np.linalg.norm(train_QFM[i])\n","        Psi_sum += np.outer(norm_state,np.conj(norm_state))\n","    #state = np.sum(train_QFM,axis=0)\n","    #norm_state = state/np.sum(state)\n","    density = Psi_sum/len(train_QFM)\n","    return density\n","\n","print(np.shape(np.outer(np.zeros(3),np.zeros(3))))\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpIiCdsvHz97"},"outputs":[],"source":["# Obtaining the density matrix\n","def mixed_density(train_QFM):\n","    rho_matrix = np.zeros((np.shape(train_QFM)[1], np.shape(train_QFM)[1]), dtype=np.complex128)\n","    for state in train_QFM:\n","        norm_state = state/np.linalg.norm(state)\n","        rho_matrix += np.outer(norm_state,np.conjugate(norm_state))\n","    rho_matrix = rho_matrix/np.shape(train_QFM)[0]\n","    return rho_matrix"]},{"cell_type":"markdown","source":["# Analysis over matrices"],"metadata":{"id":"acK1As5vkvwH"}},{"cell_type":"code","source":["def eigen(matrix):\n","    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n","    return eigenvalues, eigenvectors\n","'''''''''\n","def plot_spectra(matrix, log_xscale = True, log_yscale = False):\n","    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n","    plt.figure(figsize=(6, 4))\n","    \"\"\"\n","    plt.title('Eigenvalues distribution')\n","    plt.scatter(np.real(eigenvalues), np.ones(len(eigenvalues)), marker='o', color='b')\n","    plt.show()\n","    plt.title('Histogram')\n","    plt.hist(np.real(eigenvalues), bins=30)\n","    plt.show()\n","    \"\"\"\n","    plt.title('Absolute distribution')\n","    x_arr = np.logspace(-10, 0, 1000)\n","    abs_dist = lambda x: [np.sum(eigenvalues <= x_i) for x_i in x]\n","    #print(np.shape(abs_dist(x_arr)))\n","    #print(np.shape(x_arr))\n","    #print(np.shape(np.sum(abs_dist(x_arr), axis=1)))\n","    abs_distribution = abs_dist(x_arr)\n","    plt.scatter(x_arr, abs_dist(x_arr))\n","    threshold = 0.99\n","    idx = np.argmax( abs_distribution/abs_distribution[-1]>= threshold)\n","    x_value = x_arr[idx]\n","    plt.axvline(x=x_value, color='r', linestyle='--', label=f'{threshold*100:.1f}% Threshold at x={x_value:.4f}')\n","    plt.legend()\n","    if log_xscale: plt.xscale('log')\n","    if log_yscale: plt.yscale('log')\n","    plt.xlabel('Eigenvalue')\n","    plt.ylabel('Number of Eigenvalues')\n","    plt.show()\n","    #return eigenvalues, eigenvectors\n","'''''''''"],"metadata":{"id":"lOspSB-Br8pO","executionInfo":{"status":"ok","timestamp":1737214564993,"user_tz":300,"elapsed":96,"user":{"displayName":"Hector Julian Gutierrez Hoyos","userId":"09358593715852133437"}},"colab":{"base_uri":"https://localhost:8080/","height":192},"outputId":"941fabd1-98a7-47ed-eb2a-9fdc0392312d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef plot_spectra(matrix, log_xscale = True, log_yscale = False):\\n    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\\n    plt.figure(figsize=(6, 4))\\n    \"\"\"\\n    plt.title(\\'Eigenvalues distribution\\')\\n    plt.scatter(np.real(eigenvalues), np.ones(len(eigenvalues)), marker=\\'o\\', color=\\'b\\')\\n    plt.show()\\n    plt.title(\\'Histogram\\')\\n    plt.hist(np.real(eigenvalues), bins=30)\\n    plt.show()\\n    \"\"\"\\n    plt.title(\\'Absolute distribution\\')\\n    x_arr = np.logspace(-10, 0, 1000)\\n    abs_dist = lambda x: [np.sum(eigenvalues <= x_i) for x_i in x]\\n    #print(np.shape(abs_dist(x_arr)))\\n    #print(np.shape(x_arr))\\n    #print(np.shape(np.sum(abs_dist(x_arr), axis=1)))\\n    abs_distribution = abs_dist(x_arr)\\n    plt.scatter(x_arr, abs_dist(x_arr))\\n    threshold = 0.99\\n    idx = np.argmax( abs_distribution/abs_distribution[-1]>= threshold)\\n    x_value = x_arr[idx]\\n    plt.axvline(x=x_value, color=\\'r\\', linestyle=\\'--\\', label=f\\'{threshold*100:.1f}% Threshold at x={x_value:.4f}\\')\\n    plt.legend()\\n    if log_xscale: plt.xscale(\\'log\\')\\n    if log_yscale: plt.yscale(\\'log\\')\\n    plt.xlabel(\\'Eigenvalue\\')\\n    plt.ylabel(\\'Number of Eigenvalues\\')\\n    plt.show()\\n    #return eigenvalues, eigenvectors\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["np.sum(np.array([1,2,3,4]) <= 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkvSt3eyDlk2","executionInfo":{"status":"ok","timestamp":1737214591584,"user_tz":300,"elapsed":360,"user":{"displayName":"Hector Julian Gutierrez Hoyos","userId":"09358593715852133437"}},"outputId":"826a280f-2b75-421d-f8a7-db170115370b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["def plot_spectra(matrix, log_yscale=False, trunc_at = None):\n","    '''\n","    matrix to plot the spectra\n","    log_yscale = False, True if want to try potential\n","    trunc_at = None, from zero to one, fraction of eigenvalues to plot\n","    '''\n","    # Calculate eigenvalues and eigenvectors\n","    eigenvalues, _ = np.linalg.eigh(matrix)\n","\n","    # Set up the plot\n","    plt.figure(figsize=(6, 4))\n","    plt.title('Absolute Distribution')\n","\n","    # Absolute distribution function\n","    x_arr = np.logspace(-10, 0, 1000)\n","    abs_dist = lambda x: np.array([np.sum(eigenvalues <= x_i) for x_i in x])\n","    abs_distribution = abs_dist(x_arr)\n","\n","    if trunc_at is not None:\n","        trunc_int = int(trunc_at*len(x_arr))\n","        x_arr = x_arr[:trunc_int]\n","        abs_distribution = abs_distribution[:trunc_int]\n","\n","    # Scatter plot of the distribution\n","    plt.scatter(x_arr, abs_distribution, label='Data', alpha=0.7)\n","\n","    if log_yscale:\n","        # Log-log regression: y = ax^b -> log(y) = log(a) + b*log(x)\n","        log_x = np.log(x_arr)\n","        log_y = np.log(abs_distribution)\n","        popt, pcov = np.polyfit(log_x, log_y, 1, cov=True) # Linear fit in log-log space\n","        a = np.exp(popt[1])\n","        b = popt[0]\n","        regression_label = f'y = {a:.2e} * x^{b:.2f}'\n","        plt.plot(x_arr, a * x_arr**b, label=f'Fit: {regression_label}', color='r', linestyle='-')\n","\n","        # Calculate uncertainties\n","        a_uncertainty = np.sqrt(pcov[1, 1]) * np.exp(popt[1])\n","        b_uncertainty = np.sqrt(pcov[0, 0])\n","\n","        print(f\"a = {a:.2e} ± {a_uncertainty:.2e}\")\n","        print(f\"b = {b:.2f} ± {b_uncertainty:.2f}\")\n","\n","        # Correlation\n","        correlation_coefficient = np.corrcoef(log_x, log_y)[0, 1]\n","        print(f\"Correlation coefficient (r) = {correlation_coefficient:.5f}\")\n","\n","    if not log_yscale:\n","        # Log-linear regression: y = ae^(bx) -> log(y) = log(a) + bx\n","        log_x = np.log(x_arr)\n","        popt, pcov = np.polyfit(log_x, abs_distribution, 1, cov=True)  # Linear fit in log(y) vs x\n","        a = popt[1]\n","        b = popt[0]\n","\n","        regression_label = f'y = {b:.2e} * log(x) + {a:.2e}'\n","        plt.plot(x_arr, b * log_x + a, label=f'Fit: {regression_label}', color='r', linestyle='-')\n","\n","\n","        # Calculate uncertainties\n","        a_uncertainty = np.sqrt(pcov[1, 1])\n","        b_uncertainty = np.sqrt(pcov[0, 0])\n","\n","        print(f\"a = {a:.2f} ± {a_uncertainty:.2f}\")\n","        print(f\"b = {b:.2f} ± {b_uncertainty:.2f}\")\n","\n","        # Correlation\n","        correlation_coefficient = np.corrcoef(log_x, abs_distribution)[0, 1]\n","        print(f\"Correlation coefficient (r) = {correlation_coefficient:.5f}\")\n","    \"\"\"\n","    # Threshold line\n","    threshold = 0.99\n","    idx = np.argmax(abs_distribution / abs_distribution[-1] >= threshold)\n","    x_value = x_arr[idx]\n","    plt.axvline(x=x_value, color='g', linestyle='--', label=f'{threshold*100:.1f}% Threshold at x={x_value:.4f}')\n","    \"\"\"\n","    # Set scale, labels, and legend\n","    plt.xscale('log')\n","    if log_yscale: plt.yscale('log')\n","    plt.xlabel('Eigenvalue')\n","    plt.ylabel('Number of Eigenvalues')\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"TkyWU6otlEQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trace(matrix):\n","    return np.trace(matrix)"],"metadata":{"id":"LHqHXUdR0YQD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0qdrrcGJF_xC"},"source":["# Measurement functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOBSTH9U8lw-"},"outputs":[],"source":["def make_QFM(quantum_feature_map, N_qfm, param_qfm, train_arr, train_lb, test_arr):\n","    ## Quantum Feature Map of the train and test sets\n","    train_QFM_x = np.array([quantum_feature_map(train_point,N_qfm,param_qfm) for train_point in train_arr])\n","    train_QFM_x0 = train_QFM_x[np.where(train_lb == 0)]\n","    train_QFM_x1 = train_QFM_x[np.where(train_lb == 1)]\n","\n","    test_QFM_x = np.array([quantum_feature_map(test_point,N_qfm,param_qfm) for test_point in test_arr])\n","\n","    return train_QFM_x0, train_QFM_x1, test_QFM_x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSGy-6ppTIOO"},"outputs":[],"source":["def make_grid(quantum_feature_map, N_qfm, param_qfm,N_grid):\n","    # Generate grid of x and y values\n","    xgrid = np.linspace(0, 1, N_grid)\n","    ygrid = np.linspace(0, 1, N_grid)\n","    X, Y = np.meshgrid(xgrid, ygrid)  # Create a 2D grid\n","\n","    # Flatten the meshgrid arrays to pair all (x, y) points\n","    grid_xy = np.column_stack((X.ravel(), Y.ravel()))\n","    #print(grid_xy[0])\n","\n","    # Compute the qfm for each (x, y) pair\n","    grid_QFM = np.array([quantum_feature_map(point_xy,N_qfm,param_qfm) for point_xy in grid_xy])\n","    #print('QFM of grid')\n","    #print(grid_QFM[:10])\n","\n","    return grid_QFM, grid_xy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2T98ipJAgm_"},"outputs":[],"source":["def measurement_pure( train_QFM_x0, train_QFM_x1, test_QFM_x, test_lb, grid_QFM):\n","\n","    ## Make the pure state\n","    pstate_x0 = pure_state(train_QFM_x0)\n","    pstate_x1 = pure_state(train_QFM_x1)\n","    #print(pstate_x0)\n","\n","    #P_x0 = np.zeros(len(test_QFM_x))\n","    #P_x1 = np.zeros(len(test_QFM_x))\n","    # Compute the dot products in a vectorized manner\n","    P_x0 = np.abs(np.dot(test_QFM_x.conj(), pstate_x0))**2\n","    P_x1 = np.abs(np.dot(test_QFM_x.conj(), pstate_x1))**2\n","\n","    # Normalize the probabilities\n","    P_norm = P_x0 + P_x1\n","    P_x0_nrmld = P_x0 / P_norm\n","    P_x1_nrmld = P_x1 / P_norm\n","\n","    #Lets check the nan values and find the accuracy without them\n","    nan_mask = np.isnan(P_x0_nrmld) | np.isnan(P_x1_nrmld) # I think is not neccesary the |\n","    if np.sum(nan_mask)>0: print(f'There are {np.sum(nan_mask)} nan values')\n","\n","    P_x0_nrmld = P_x0_nrmld[~nan_mask]\n","    P_x1_nrmld = P_x1_nrmld[~nan_mask]\n","    test_lb = test_lb[~nan_mask]\n","\n","    # Now\n","    predicted_labels = np.array(P_x1_nrmld>0.5).astype(int)\n","    #print(predicted_labels)\n","\n","    # Calculate accuracy\n","    accuracy = np.mean(predicted_labels == test_lb)\n","    #print(f\"Accuracy: {accuracy}\")\n","    \"\"\"\n","    #print(f\"Accuracy: {accuracy}\")\n","    print(f'N of predicted = {np.sum(predicted_labels == test_lb)}.')\n","    print(f'N of test = {len(test_lb)}.')\n","    print(f'Length of predicted = {len(predicted_labels)}.')\n","    print(np.array(predicted_labels==test_lb).astype(int))\n","    print(np.arange(len(test_lb))[predicted_labels!=test_lb])\n","    \"\"\"\n","    ## Plot\n","    #P_x0_grid = np.zeros(len(grid_QFM))\n","    #P_x1_grid = np.zeros(len(grid_QFM))\n","    # Compute the dot products in a vectorized manner for grid\n","    P_x0_grid = np.abs(np.dot(grid_QFM.conj(), pstate_x0))**2\n","    P_x1_grid = np.abs(np.dot(grid_QFM.conj(), pstate_x1))**2\n","\n","    return P_x0, P_x1, P_x0_grid, P_x1_grid, accuracy"]},{"cell_type":"code","source":["#np.dot(np.array([[1,2,3],[1,2,4]]),np.array([1,2,3]))"],"metadata":{"id":"BH77xInM0SgV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_M9F10yuAKhE"},"outputs":[],"source":["def measurement_mixed( train_QFM_x0, train_QFM_x1, test_QFM_x, test_lb, grid_QFM, density_x0 = None, density_x1 = None):\n","\n","    if density_x0 is None and density_x1 is None:\n","        mdensity_x0 = mixed_density(train_QFM_x0)\n","        mdensity_x1 = mixed_density(train_QFM_x1)\n","    else:\n","        mdensity_x0 = density_x0\n","        mdensity_x1 = density_x1\n","\n","    #P_x0 = np.zeros(len(test_QFM_x))\n","    #P_x1 = np.zeros(len(test_QFM_x))\n","    # Compute Pm_x0 and Pm_x1 in a vectorized manner (be careful)\n","    P_x0 = np.abs(np.einsum('ij,ij->i', np.dot(test_QFM_x.conj(), mdensity_x0), test_QFM_x))\n","    P_x1 = np.abs(np.einsum('ij,ij->i', np.dot(test_QFM_x.conj(), mdensity_x1), test_QFM_x))\n","\n","    \"\"\"\n","    # The old version\n","\n","    # Initialize arrays to store the results\n","    P_x0_grid = np.zeros(len(grid_QFM))\n","    P_x1_grid = np.zeros(len(grid_QFM))\n","\n","    # Compute the double dot product for each vector v in grid_QFM\n","    for i, v in enumerate(grid_QFM):\n","        P_x0_grid[i] = np.abs(np.vdot(v, np.dot(mdensity_x0, v)))\n","        P_x1_grid[i] = np.abs(np.vdot(v, np.dot(mdensity_x1, v)))\n","    \"\"\"\n","    # Normalize probabilities\n","    normP = (P_x0 + P_x1)\n","    P_x0_nrmld  = P_x0 / normP\n","    P_x1_nrmld  = P_x1 / normP\n","\n","    #Lets check the nan values and find the accuracy without them\n","    nan_mask = np.isnan(P_x0_nrmld) | np.isnan(P_x1_nrmld) # I think is not neccesary the |\n","    if np.sum(nan_mask)>0: print(f'There are {np.sum(nan_mask)} nan values')\n","\n","    P_x0_nrmld = P_x0_nrmld[~nan_mask]\n","    P_x1_nrmld = P_x1_nrmld[~nan_mask]\n","    test_lb = test_lb[~nan_mask]\n","\n","    # Now\n","    predicted_labels = np.array(P_x1_nrmld>0.5).astype(int)\n","    #print(predicted_labels)\n","\n","    # Calculate accuracy\n","    accuracy = np.mean(predicted_labels == test_lb)\n","    #print(f\"Accuracy: {accuracy}\")\n","    \"\"\"\n","    #print(f\"Accuracy: {accuracy}\")\n","    print(f'N of predicted = {np.sum(predicted_labels == test_lb)}.')\n","    print(f'N of test = {len(test_lb)}.')\n","    print(f'Length of predicted = {len(predicted_labels)}.')\n","    print(np.array(predicted_labels==test_lb).astype(int))\n","    print(np.arange(len(test_lb))[predicted_labels!=test_lb])\n","    \"\"\"\n","    ## Plot\n","    #P_x0_grid = np.zeros(len(grid_QFM))\n","    #P_x1_grid = np.zeros(len(grid_QFM))\n","    # Compute the dot products in a vectorized manner for grid\n","    P_x0_grid = np.abs(np.einsum('ij,ij->i', np.dot(grid_QFM.conj(), mdensity_x0), grid_QFM))\n","    P_x1_grid = np.abs(np.einsum('ij,ij->i', np.dot(grid_QFM.conj(), mdensity_x1), grid_QFM))\n","\n","    # Be careful, the conjugate must be in the first grid_QFM, I don't know why\n","\n","\n","    return P_x0, P_x1, P_x0_grid, P_x1_grid, accuracy, mdensity_x0, mdensity_x1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8ujrJU6bD1y"},"outputs":[],"source":["def measurement_diagonal( train_QFM_x0, train_QFM_x1, test_QFM_x, test_lb, grid_QFM, density_x0 = None, density_x1 = None):\n","\n","    if density_x0 is None and density_x1 is None:\n","        density_x0 = mixed_density(train_QFM_x0)\n","        density_x1 = pure_density(train_QFM_x1)\n","\n","    mdiagonal_x0 = density_x0.diagonal()\n","    mdiagonal_x1 = density_x1.diagonal()\n","\n","    # Compute the dot products in a vectorized manner\n","    squared_test_QFM_x = np.square(np.abs(test_QFM_x))\n","    P_x0 = np.abs(np.dot(squared_test_QFM_x, mdiagonal_x0))\n","    P_x1 = np.abs(np.dot(squared_test_QFM_x, mdiagonal_x1))\n","\n","    # Normalize probabilities\n","    normP = (P_x0 + P_x1)\n","    P_x0_nrmld  = P_x0 / normP\n","    P_x1_nrmld  = P_x1 / normP\n","\n","    #Lets check the nan values and find the accuracy without them\n","    nan_mask = np.isnan(P_x0_nrmld) | np.isnan(P_x1_nrmld) # I think is not neccesary the |\n","    if np.sum(nan_mask)>0: print(f'There are {np.sum(nan_mask)} nan values')\n","\n","    P_x0_nrmld = P_x0_nrmld[~nan_mask]\n","    P_x1_nrmld = P_x1_nrmld[~nan_mask]\n","    test_lb = test_lb[~nan_mask]\n","\n","    # Now\n","    predicted_labels = np.array(P_x1_nrmld>=0.5).astype(int)\n","    #print(predicted_labels)\n","    #accuracy = 0\n","    # Calculate accuracy\n","    accuracy = np.mean(predicted_labels == test_lb)\n","    \"\"\"\n","    #print(f\"Accuracy: {accuracy}\")\n","    print(f'N of predicted = {np.sum(predicted_labels == test_lb)}.')\n","    print(f'N of test = {len(test_lb)}.')\n","    print(f'Length of predicted = {len(predicted_labels)}.')\n","    print(f'Is predicted? {np.array(predicted_labels==test_lb).astype(int)}')\n","    print(f'Which bad predicted {np.arange(len(test_lb))[predicted_labels!=test_lb]}')\n","    print(f'Predicted labels {predicted_labels}')\n","    #print(f'Test labels {test_lb}')\n","    #print(f'test_QFM {test_QFM_x}')\n","    #print(f'squared_test_QFM_x {squared_test_QFM_x}')\n","    print(f'mdiagonal_x0 {mdiagonal_x0}')\n","    print(f'mdiagonal_x1 {mdiagonal_x1}')\n","    \"\"\"\n","\n","    ## Plot\n","    P_x0_grid = np.zeros(len(grid_QFM))\n","    P_x1_grid = np.zeros(len(grid_QFM))\n","\n","\n","    # Compute the dot products in a vectorized manner for grid\n","    squared_grid_QFM_x = np.square(np.abs(grid_QFM))\n","    P_x0_grid = np.abs(np.dot(squared_grid_QFM_x, mdiagonal_x0))\n","    P_x1_grid = np.abs(np.dot(squared_grid_QFM_x, mdiagonal_x1))\n","\n","    return P_x0, P_x1, P_x0_grid, P_x1_grid, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeJTEKHCUvSi"},"outputs":[],"source":["def plot_clasification(grid_xy, test_arr, test_lb, P_x0, P_x1, accuracy, dataset, quantum_feature_map, type_est):\n","    ## Individual\n","    # Create a red, white, blue colormap\n","    cmap_blue = LinearSegmentedColormap.from_list('white_blue', ['white', 'blue'])\n","    colors_blue = P_x0\n","    cmap_red = LinearSegmentedColormap.from_list('white_red', ['white', 'red'])\n","    colors_red = P_x1\n","\n","    # Scatter plot with the proper grid\n","    plt.figure(figsize=(9, 4))\n","    plt.subplot(121)\n","    sc1= plt.scatter(grid_xy[:, 0],grid_xy[:, 1], c=colors_blue,\n","                cmap=cmap_blue, marker='.')\n","    plt.colorbar(sc1)\n","    plt.scatter(test_arr[test_lb ==0, 0], test_arr[test_lb ==0, 1],\n","                c = 'b', edgecolor='k', label=dataset)\n","    plt.xlabel('x')\n","    plt.ylabel('y')\n","    plt.subplot(122)\n","    sc2 = plt.scatter(grid_xy[:, 0], grid_xy[:, 1], c=colors_red,\n","                cmap=cmap_red, marker='.')\n","    plt.colorbar(sc2)\n","    plt.scatter(test_arr[test_lb ==1, 0], test_arr[test_lb ==1, 1],\n","                c = 'r', edgecolor='k', label=dataset)\n","    plt.xlabel('x')\n","    plt.suptitle('Individual '+ (quantum_feature_map.__name__) + ' ' + type_est)\n","    plt.show()\n","\n","    Pnorm_x0 = P_x0 / (P_x0 + P_x1)\n","    ## Joint\n","    # Create a red, white, blue colormap\n","    cmap = LinearSegmentedColormap.from_list('red_white_blue', ['red', 'white', 'blue'])\n","    colors = Pnorm_x0\n","\n","    # Scatter plot with the proper grid\n","    plt.figure(figsize=(6, 6))\n","    sc = plt.scatter(grid_xy[:, 0], grid_xy[:, 1], c=colors, cmap=cmap)\n","\n","    plt.scatter(test_arr[:, 0], test_arr[:, 1], c=test_lb, cmap='bwr', edgecolor='k', label=dataset)\n","\n","    # Annotate all test points with indices\n","    #for i, (x, y) in enumerate(test_arr):\n","    #    plt.text(x, y, str(i), fontsize=8, color='black')\n","\n","    # Labels and title\n","    plt.xlabel('x')\n","    plt.ylabel('y')\n","    plt.title('Joint '+ (quantum_feature_map.__name__) + ' ' + type_est+ ' Accuracy: '+str(accuracy))\n","\n","    # Show the plot\n","    plt.show()"]},{"cell_type":"markdown","source":["# Reduce Matrix"],"metadata":{"id":"gunqKcibrjZE"}},{"cell_type":"code","source":["'''''''''\n","\n","def reduce_matrix(matrix):\n","    # Step 1: Compute eigenvalues and eigenvectors\n","    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n","\n","    # Step 2: Truncate eigenvalues below a threshold\n","    threshold = 2.0\n","    reduced_eigenvalues = np.where(eigenvalues > threshold, eigenvalues, 0)\n","\n","    # Step 3: Reconstruct the matrix\n","    # D is the diagonal matrix of reduced eigenvalues\n","    D_reduced = np.diag(reduced_eigenvalues)\n","    matrix_reduced = eigenvectors @ D_reduced @ eigenvectors.T\n","    return matrix_reduced\n","\n","'''''''''"],"metadata":{"id":"GdIlRY4-rot2","executionInfo":{"status":"ok","timestamp":1737214564997,"user_tz":300,"elapsed":82,"user":{"displayName":"Hector Julian Gutierrez Hoyos","userId":"09358593715852133437"}},"colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"0bc790fb-4ada-4f89-fca7-0de961054e26"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n\\ndef reduce_matrix(matrix):\\n    # Step 1: Compute eigenvalues and eigenvectors\\n    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\\n\\n    # Step 2: Truncate eigenvalues below a threshold\\n    threshold = 2.0\\n    reduced_eigenvalues = np.where(eigenvalues > threshold, eigenvalues, 0)\\n\\n    # Step 3: Reconstruct the matrix\\n    # D is the diagonal matrix of reduced eigenvalues\\n    D_reduced = np.diag(reduced_eigenvalues)\\n    matrix_reduced = eigenvectors @ D_reduced @ eigenvectors.T\\n    return matrix_reduced\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["def reduce_matrix(matrix, threshold = 0):\n","    '''\n","    threshold = 0,  between 0 and 1\n","    '''\n","    # Step 1: Compute eigenvalues and eigenvectors\n","    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n","\n","    # Step 2: Truncate eigenvalues below a threshold\n","    max_eigenvalue = np.max(eigenvalues)\n","    threshold2 = max_eigenvalue * threshold\n","    reduced_eigenvalues = np.where(eigenvalues > threshold2, eigenvalues, 0)\n","    ### The commented code below were just to try an hypotesis\n","    #min = 0.1\n","    #max = 1\n","    #mask = (eigenvalues >= min*max_eigenvalue) & (eigenvalues <= max*max_eigenvalue)\n","    #reduced_eigenvalues = np.where(mask, eigenvalues, 0)\n","    nonzero_indices = np.nonzero(reduced_eigenvalues)\n","    print(f'Matrix reduced from dimension {len(eigenvalues)} to {len(nonzero_indices[0])}')\n","    #print(f'Eigenvalues {eigenvalues}')\n","    # Step 3: Reconstruct the matrix\n","    # D is the diagonal matrix of reduced eigenvalues\n","    #reduced_eigenvalues = eigenvalues\n","    D_reduced = np.diag(reduced_eigenvalues)\n","    matrix_reduced = eigenvectors @ D_reduced @ eigenvectors.T.conj()\n","    return matrix_reduced"],"metadata":{"id":"lVqtR2NtmRCQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["pMr3MGypKrXJ","kPpkyfHGnizR","J_pZ7tEWt7om","T6h_tdkj8AXL","acK1As5vkvwH"],"authorship_tag":"ABX9TyN00b1/U8dGryH5malAHIr6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}